{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "def load_data(dataset_folders):\n",
    "    combined_data = []\n",
    "    for folder in dataset_folders:  # Update with the path to your datasets\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                filepath = os.path.join(folder, filename)\n",
    "                # print(f\"Loading {filepath}\")\n",
    "                participant_data = pd.read_csv(filepath)\n",
    "                combined_data.append(participant_data)\n",
    "    return pd.concat(combined_data, ignore_index=True)\n",
    "\n",
    "param_grid_lr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'lbfgs'],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "param_grid_dt = {\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "    'class_weight': [None, 'balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.001],\n",
    "    # 'max_depth': [3, 6, 9],\n",
    "    # 'subsample': [0.5, 0.7, 1.0],\n",
    "    # 'colsample_bytree': [0.5, 0.7, 1.0],\n",
    "    # 'gamma': [0, 0.1, 0.2],\n",
    "    # 'reg_lambda': [0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "def perform_grid_search(estimator, param_grid, X, y, scoring='accuracy', cv=5):\n",
    "    grid_search = GridSearchCV(estimator, param_grid, scoring=scoring, cv=cv, verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X, y)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best score:\", grid_search.best_score_)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def train_test_split_by_participant(data, test_size=0.2):\n",
    "    \"\"\"Split data by participant for age and gender models.\"\"\"\n",
    "    participants = data['Participant'].unique()\n",
    "    np.random.shuffle(participants)\n",
    "    test_size = int(len(participants) * test_size)\n",
    "    test_participants = participants[-test_size:]\n",
    "    train_participants = participants[:-test_size]\n",
    "\n",
    "    train_data = data[data['Participant'].isin(train_participants)]\n",
    "    test_data = data[data['Participant'].isin(test_participants)]\n",
    "\n",
    "    # Detailed split information\n",
    "    print(\"Training on participants from databases:\\n\", train_data[['Participant', 'Database']].drop_duplicates())\n",
    "    print(\"Testing on participants from databases:\\n\", test_data[['Participant', 'Database']].drop_duplicates())\n",
    "    return train_data, test_data\n",
    "\n",
    "def time_series_split(data, test_ratio=0.2):\n",
    "    \"\"\"Time-series split for participant ID identification.\"\"\"\n",
    "    train_data = data.groupby('Participant').apply(lambda x: x.iloc[:int(len(x)*(1-test_ratio))]).reset_index(drop=True)\n",
    "    test_data = data.groupby('Participant').apply(lambda x: x.iloc[int(len(x)*(1-test_ratio)):]).reset_index(drop=True)\n",
    "    \n",
    "    # Detailed split information\n",
    "    for participant, group in data.groupby('Participant'):\n",
    "        train_end = int(len(group)*(1-test_ratio))\n",
    "        # print(f\"Participant {participant} from {group['Database'].iloc[0]}: Train indices 0 to {train_end}, Test indices {train_end} to {len(group)}\")\n",
    "    return train_data, test_data\n",
    "\n",
    "def prepare_features_labels(data, label_column, encode=True):\n",
    "    \"\"\"Prepare features and labels for training.\"\"\"\n",
    "    features = data.drop(columns=['Participant', 'Sample', 'Sampling_Rate', 'Database', 'Gender', 'Age', 'age_binned'])\n",
    "    labels = data[label_column]\n",
    "    if encode:\n",
    "        encoder = LabelEncoder()\n",
    "        labels = encoder.fit_transform(labels)\n",
    "    return features, labels\n",
    "\n",
    "def run_shap_analysis(model, X_train, X_test, model_name, task, aggregate_classes=False):\n",
    "    # Sample a subset of data if X_test is too large\n",
    "    X_test_sample = X_train.sample(n=1000, random_state=42) if len(X_test) > 1000 else X_test\n",
    "    \n",
    "    # Choose the right explainer based on model type\n",
    "    if model_name == 'Logistic Regression':\n",
    "        explainer = shap.LinearExplainer(model=model, data=X_train, masker=shap.maskers.Independent(data=X_train))\n",
    "        shap_values = explainer.shap_values(X_test_sample)\n",
    "    elif model_name in ['Decision Tree', 'Random Forest', 'XGBoost']:\n",
    "        explainer = shap.TreeExplainer(model=model, data=X_train)\n",
    "        shap_values = explainer.shap_values(X_test_sample, check_additivity=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type {model_name}\")\n",
    "    \n",
    "    \n",
    "    feature_names = X_test_sample.columns.tolist()\n",
    "\n",
    "    # Handle multi-class outputs\n",
    "    if len(shap_values.shape) == 3:\n",
    "        if aggregate_classes:\n",
    "            # Aggregate SHAP values across classes\n",
    "            aggregated_shap_values = np.mean(np.abs(shap_values), axis=2)\n",
    "            aggregated_shap_values = np.mean(shap_values, axis=2)\n",
    "            plot_shap_summary(aggregated_shap_values, X_test_sample, model_name, task, feature_names=feature_names)\n",
    "        else:\n",
    "            # Visualize each class separately\n",
    "            for i in range(shap_values.shape[2]):\n",
    "                plot_shap_summary(shap_values[:, :, i], X_test_sample, model_name, task, class_index=i, feature_names=feature_names)\n",
    "                # plot_decision_plot(explainer.expected_value[i], shap_values[:, :, i], feature_names, model_name, task, class_index=i)\n",
    "    else:\n",
    "        plot_shap_summary(shap_values, X_test_sample, model_name, task, feature_names=feature_names)\n",
    "        # plot_decision_plot(explainer.expected_value, shap_values, feature_names, model_name, task)\n",
    "\n",
    "def plot_shap_summary(shap_values, features, model_name, task, class_index=None, feature_names=None):\n",
    "    suffix = f\"_class_{class_index}\" if class_index is not None else \"\"\n",
    "    plt.figure(figsize=(40, 12))\n",
    "    shap.summary_plot(shap_values, features=features, feature_names=feature_names, plot_type=\"bar\", show=False)\n",
    "    plt.title(f'SHAP Summary Plot (Bar) - {model_name} for {task}{suffix}')\n",
    "    plt.savefig(f'plot/{model_name}_{task}{suffix}_shap_summary_bar.pdf')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(40, 12))\n",
    "    shap.summary_plot(shap_values, features=features, feature_names=feature_names, show=False)\n",
    "    plt.title(f'SHAP Detailed Summary Plot - {model_name} for {task}{suffix}')\n",
    "    plt.savefig(f'plot/{model_name}_{task}{suffix}_shap_detailed.pdf')\n",
    "    plt.close()\n",
    "\n",
    "def plot_decision_plot(base_value, shap_values, feature_names, model_name, task, class_index=None):\n",
    "    suffix = f\"_class_{class_index}\" if class_index is not None else \"\"\n",
    "    plt.figure(figsize=(40, 12))\n",
    "    shap.decision_plot(base_value, shap_values, feature_names=feature_names, show=False)\n",
    "    plt.title(f'Decision Plot - {model_name} for {task}{suffix} (Sample Instances)')\n",
    "    plt.savefig(f'plot/{model_name}_{task}{suffix}_decision_plot.pdf')\n",
    "    plt.close()\n",
    "\n",
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, n=1):\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "    best_predictions = None\n",
    "    best_probabilities = None\n",
    "    best_roc_auc = None\n",
    "    for _ in range(n):\n",
    "        cloned_model = clone(model)\n",
    "        cloned_model.fit(X_train, y_train)\n",
    "        predictions = cloned_model.predict(X_test)\n",
    "        probabilities = cloned_model.predict_proba(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        # try:\n",
    "        #     if probabilities.shape[1] == 2:\n",
    "        #         roc_auc = roc_auc_score(y_test, probabilities[:, 1])\n",
    "        #     else:\n",
    "        #         # Binarize the output\n",
    "        #         y_test_bin = label_binarize(y_test, classes=classes)\n",
    "        #         roc_auc = roc_auc_score(y_test_bin, probabilities, multi_class='ovr')\n",
    "        # except Exception as e:\n",
    "        #     roc_auc = None\n",
    "        #     print(f\"ROC AUC could not be calculated: {e}\")\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = cloned_model\n",
    "            best_predictions = predictions\n",
    "            best_probabilities = probabilities\n",
    "            \n",
    "    return best_model, best_accuracy, best_predictions, best_probabilities\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name, task):\n",
    "    # model.fit(X_train, y_train)\n",
    "    \n",
    "    model, best_accuracy, predictions, probabilities = train_and_evaluate(model, X_train, y_train, X_test, y_test)\n",
    "    print(f\"Selected best model with accuracy: {best_accuracy:.4f}\")\n",
    "    # predictions = model.predict(X_test)\n",
    "    # probabilities = model.predict_proba(X_test)\n",
    "    classes = np.unique(np.concatenate([y_train, y_test]))\n",
    "\n",
    "    # print(f\"Shapes - Predictions: {predictions.shape}, Probabilities: {probabilities.shape}\")\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='weighted')\n",
    "    recall = recall_score(y_test, predictions, average='weighted')\n",
    "    f1 = f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "    try:\n",
    "        if probabilities.shape[1] == 2:\n",
    "            roc_auc = roc_auc_score(y_test, probabilities[:, 1])\n",
    "        else:\n",
    "            # Binarize the output\n",
    "            y_test_bin = label_binarize(y_test, classes=classes)\n",
    "            roc_auc = roc_auc_score(y_test_bin, probabilities, multi_class='ovr')\n",
    "    except Exception as e:\n",
    "        roc_auc = None\n",
    "        print(f\"ROC AUC could not be calculated: {e}\")\n",
    "\n",
    "    print(\"Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    if roc_auc:\n",
    "        print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    if task == 'Participant ID':\n",
    "        plt.figure(figsize=(72, 54))\n",
    "    else: \n",
    "        plt.figure(figsize=(12, 9))\n",
    "        \n",
    "    threshold = np.percentile(cm, 0)  # Adjust the percentile threshold\n",
    "    mask = cm < threshold\n",
    "    masked_confusion_matrix = np.where(mask, np.nan, cm)  \n",
    "    sns.heatmap(masked_confusion_matrix, annot=True, fmt='.2f', cmap='Blues', xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))\n",
    "    plt.title(f'Confusion Matrix for {model_name} on {task}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(f'plot/Confusion_Matrix_{model_name}_{task}.pdf')  # Save the confusion matrix figure\n",
    "    plt.show()\n",
    "\n",
    "    if probabilities.shape[1] == 2:  # Only plot ROC for binary classification\n",
    "        fpr, tpr, _ = roc_curve(y_test, probabilities[:, 1])\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label='ROC Curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], 'r--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve for {model_name} on {task}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(f'plot/{model_name}_{task}_ROC_Curve.pdf')  # Save the confusion matrix figure\n",
    "        plt.show()\n",
    "        \n",
    "    # SHAP analysis\n",
    "    try:\n",
    "        # run_shap_analysis(model, X_train, X_test, model_name, task)\n",
    "        run_shap_analysis(model, X_train, X_test, model_name, task, aggregate_classes=True)\n",
    "    except Exception as e:\n",
    "        print(f\"SHAP analysis failed on {task} with model {model_name}\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main experiment setup\n",
    "dataset_paths = ['mit-bih_features/', 'mit-long_features/', 'smart_features/', 'chfdb_features/', 'brno_features/']\n",
    "combined_data = load_data(dataset_paths)\n",
    "# drop the 'Half Minute' column\n",
    "combined_data = combined_data.drop(columns=['Half Minute'])\n",
    "combined_data['Gender'] = combined_data['Gender'].replace({'m': 'M'})\n",
    "\n",
    "combined_data = combined_data.dropna()\n",
    "unique_genders = combined_data['Gender'].unique()\n",
    "\n",
    "age_bins = [0, 18, 35, 50, 65, 80, 100]\n",
    "age_labels = [f\"{age_bins[i]}-{age_bins[i+1]}\" for i in range(len(age_bins)-1)]\n",
    "\n",
    "combined_data['age_binned'] = pd.cut(combined_data['Age'], bins=age_bins, labels=age_labels, right=False)\n",
    "le = LabelEncoder()\n",
    "combined_data['age_binned'] = le.fit_transform(combined_data['age_binned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data for age and gender identification\n",
    "train_data_ag, test_data_ag = train_test_split_by_participant(combined_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, Flatten, Reshape, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Check for GPU and set memory growth\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Error setting memory growth: \", e)\n",
    "\n",
    "# Assume prepare_features_labels function is defined elsewhere\n",
    "X_train, y_train = prepare_features_labels(train_data_ag, 'Gender')\n",
    "X_test, y_test = prepare_features_labels(test_data_ag, 'Gender')\n",
    "\n",
    "# Building the model\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Binary output\n",
    "])\n",
    "\n",
    "# model = Sequential([\n",
    "#     Reshape((1, X_train.shape[1]), input_shape=(X_train.shape[1],)),\n",
    "#     Conv1D(64, kernel_size=1, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "#     BatchNormalization(),\n",
    "#     Conv1D(32, kernel_size=1, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "#     Flatten(),\n",
    "#     Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(16, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# model = Sequential([\n",
    "#     LSTM(64, input_shape=(2, X_train.shape[1]), return_sequences=False),  # `return_sequences=False` because the next layer is Dense\n",
    "#     Dropout(0.5),\n",
    "#     Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "# ])\n",
    "\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.005),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluating the model\n",
    "results = model.evaluate(X_test, y_test, verbose=1)\n",
    "predictions = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "probabilities = model.predict(X_test).flatten()\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "roc_auc = roc_auc_score(y_test, probabilities)\n",
    "\n",
    "print(\"Predicting gender\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"ROC AUC Score:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Age Identification ---\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/llm/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6917 - loss: 0.9873 - val_accuracy: 0.2516 - val_loss: 2.6695\n",
      "Epoch 2/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7021 - loss: 0.9334 - val_accuracy: 0.2516 - val_loss: 2.5853\n",
      "Epoch 3/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6979 - loss: 0.9464 - val_accuracy: 0.2516 - val_loss: 2.5368\n",
      "Epoch 4/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7002 - loss: 0.9423 - val_accuracy: 0.2516 - val_loss: 2.5909\n",
      "Epoch 5/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6975 - loss: 0.9437 - val_accuracy: 0.2516 - val_loss: 2.5643\n",
      "Epoch 6/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6970 - loss: 0.9426 - val_accuracy: 0.2516 - val_loss: 2.7261\n",
      "Epoch 7/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7005 - loss: 0.9364 - val_accuracy: 0.2516 - val_loss: 2.7612\n",
      "Epoch 8/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6972 - loss: 0.9448 - val_accuracy: 0.2516 - val_loss: 2.7375\n",
      "Epoch 9/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6989 - loss: 0.9398 - val_accuracy: 0.2516 - val_loss: 2.5438\n",
      "Epoch 10/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7055 - loss: 0.9264 - val_accuracy: 0.2516 - val_loss: 2.6388\n",
      "Epoch 11/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6996 - loss: 0.9376 - val_accuracy: 0.2516 - val_loss: 2.6872\n",
      "Epoch 12/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7002 - loss: 0.9367 - val_accuracy: 0.2516 - val_loss: 2.5619\n",
      "Epoch 13/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6957 - loss: 0.9430 - val_accuracy: 0.2516 - val_loss: 2.5664\n",
      "Epoch 14/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7024 - loss: 0.9350 - val_accuracy: 0.2516 - val_loss: 2.6625\n",
      "Epoch 15/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7005 - loss: 0.9355 - val_accuracy: 0.2516 - val_loss: 2.6511\n",
      "Epoch 16/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7022 - loss: 0.9330 - val_accuracy: 0.2516 - val_loss: 2.6133\n",
      "Epoch 17/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6960 - loss: 0.9434 - val_accuracy: 0.2516 - val_loss: 2.7203\n",
      "Epoch 18/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7003 - loss: 0.9360 - val_accuracy: 0.2516 - val_loss: 2.6506\n",
      "Epoch 19/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6923 - loss: 0.9511 - val_accuracy: 0.2516 - val_loss: 2.6114\n",
      "Epoch 20/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6946 - loss: 0.9470 - val_accuracy: 0.2516 - val_loss: 2.5377\n",
      "Epoch 21/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6969 - loss: 0.9400 - val_accuracy: 0.2516 - val_loss: 2.7321\n",
      "Epoch 22/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6977 - loss: 0.9412 - val_accuracy: 0.2516 - val_loss: 2.7496\n",
      "Epoch 23/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7007 - loss: 0.9315 - val_accuracy: 0.2516 - val_loss: 2.6951\n",
      "Epoch 24/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6961 - loss: 0.9448 - val_accuracy: 0.2516 - val_loss: 2.6988\n",
      "Epoch 25/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7011 - loss: 0.9352 - val_accuracy: 0.2516 - val_loss: 2.6900\n",
      "Epoch 26/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7038 - loss: 0.9285 - val_accuracy: 0.2516 - val_loss: 2.5553\n",
      "Epoch 27/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7026 - loss: 0.9313 - val_accuracy: 0.2516 - val_loss: 2.4774\n",
      "Epoch 28/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6996 - loss: 0.9354 - val_accuracy: 0.2516 - val_loss: 2.6093\n",
      "Epoch 29/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6939 - loss: 0.9475 - val_accuracy: 0.2516 - val_loss: 2.6320\n",
      "Epoch 30/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7024 - loss: 0.9283 - val_accuracy: 0.2516 - val_loss: 2.6764\n",
      "Epoch 31/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6986 - loss: 0.9401 - val_accuracy: 0.2516 - val_loss: 2.6818\n",
      "Epoch 32/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6949 - loss: 0.9478 - val_accuracy: 0.2516 - val_loss: 2.7452\n",
      "Epoch 33/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6982 - loss: 0.9407 - val_accuracy: 0.2516 - val_loss: 2.6077\n",
      "Epoch 34/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6974 - loss: 0.9418 - val_accuracy: 0.2516 - val_loss: 2.6446\n",
      "Epoch 35/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6956 - loss: 0.9421 - val_accuracy: 0.2516 - val_loss: 2.6053\n",
      "Epoch 36/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6965 - loss: 0.9390 - val_accuracy: 0.2516 - val_loss: 2.6040\n",
      "Epoch 37/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7019 - loss: 0.9312 - val_accuracy: 0.2516 - val_loss: 2.5616\n",
      "Epoch 38/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6988 - loss: 0.9382 - val_accuracy: 0.2516 - val_loss: 2.6150\n",
      "Epoch 39/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7007 - loss: 0.9344 - val_accuracy: 0.2516 - val_loss: 2.5329\n",
      "Epoch 40/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7003 - loss: 0.9352 - val_accuracy: 0.2516 - val_loss: 2.7516\n",
      "Epoch 41/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7032 - loss: 0.9294 - val_accuracy: 0.2516 - val_loss: 2.5898\n",
      "Epoch 42/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6986 - loss: 0.9412 - val_accuracy: 0.2516 - val_loss: 2.6694\n",
      "Epoch 43/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7006 - loss: 0.9332 - val_accuracy: 0.2516 - val_loss: 2.6170\n",
      "Epoch 44/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6978 - loss: 0.9394 - val_accuracy: 0.2516 - val_loss: 2.6665\n",
      "Epoch 45/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6986 - loss: 0.9384 - val_accuracy: 0.2516 - val_loss: 2.5359\n",
      "Epoch 46/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6998 - loss: 0.9351 - val_accuracy: 0.2516 - val_loss: 2.5487\n",
      "Epoch 47/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6993 - loss: 0.9376 - val_accuracy: 0.2516 - val_loss: 2.7510\n",
      "Epoch 48/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6992 - loss: 0.9378 - val_accuracy: 0.2516 - val_loss: 2.6264\n",
      "Epoch 49/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7000 - loss: 0.9365 - val_accuracy: 0.2516 - val_loss: 2.6112\n",
      "Epoch 50/50\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6987 - loss: 0.9358 - val_accuracy: 0.2516 - val_loss: 2.7160\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - accuracy: 0.5957 - loss: 1.1694\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step\n",
      "\n",
      "--- Age Identification Results ---\n",
      "Accuracy: 0.6118548324692409\n",
      "Precision: 0.3743663360159628\n",
      "Recall: 0.6118548324692409\n",
      "F1 Score: 0.4645161939831289\n",
      "ROC AUC Score: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming age_binned is already an integer encoding of classes\n",
    "X_train_age, y_train_age = prepare_features_labels(train_data_ag, 'age_binned')\n",
    "X_test_age, y_test_age = prepare_features_labels(test_data_ag, 'age_binned')\n",
    "print(\"\\n--- Age Identification ---\")\n",
    "y_train_age = to_categorical(y_train_age)\n",
    "y_test_age = to_categorical(y_test_age)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train_age.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y_train_age.shape[1], activation='softmax')  # Multi-class output\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.05),\n",
    "              loss='categorical_crossentropy',  # Updated loss function\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_age, y_train_age, validation_split=0.2, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "results = model.evaluate(X_test_age, y_test_age, verbose=1)\n",
    "predictions = model.predict(X_test_age)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test_age, axis=1)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "precision = precision_score(true_classes, predicted_classes, average='weighted')\n",
    "recall = recall_score(true_classes, predicted_classes, average='weighted')\n",
    "f1 = f1_score(true_classes, predicted_classes, average='weighted')\n",
    "roc_auc = roc_auc_score(y_test_age, predictions, multi_class='ovr')\n",
    "\n",
    "print(\"\\n--- Age Identification Results ---\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"ROC AUC Score:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Participant ID Identification ---\n",
      "       P_minus_R_amp_mean  Q_minus_R_amp_mean  S_minus_R_amp_mean  \\\n",
      "0               -1.141249           -1.324783           -1.299768   \n",
      "1               -1.139667           -1.322945           -1.298502   \n",
      "2               -1.107483           -1.343764           -1.332647   \n",
      "3               -1.152959           -1.323557           -1.297404   \n",
      "4               -1.141500           -1.323596           -1.296803   \n",
      "...                   ...                 ...                 ...   \n",
      "13151           -0.023178           -0.408367           -0.575172   \n",
      "13152           -0.083903           -0.271396           -0.486075   \n",
      "13153           -0.046202           -0.374836           -0.554408   \n",
      "13154           -0.029998           -0.281672           -0.467778   \n",
      "13155           -0.028526           -0.242989           -0.570703   \n",
      "\n",
      "       T_minus_R_amp_mean  PQ_interval_mean  QR_interval_mean  \\\n",
      "0               -1.148128          0.170458          0.038500   \n",
      "1               -1.147751          0.170174          0.039348   \n",
      "2               -0.997635          0.157478          0.057375   \n",
      "3               -1.141717          0.171087          0.038435   \n",
      "4               -1.153769          0.172958          0.039625   \n",
      "...                   ...               ...               ...   \n",
      "13151           -0.049246          0.170313          0.034375   \n",
      "13152           -0.067961          0.196484          0.026172   \n",
      "13153           -0.027545          0.181726          0.048573   \n",
      "13154           -0.025122          0.174253          0.038723   \n",
      "13155            0.000811          0.179008          0.036345   \n",
      "\n",
      "       RS_interval_mean  ST_interval_mean  \n",
      "0              0.075458          0.210292  \n",
      "1              0.075826          0.207609  \n",
      "2              0.077667          0.200000  \n",
      "3              0.077000          0.207174  \n",
      "4              0.076958          0.206042  \n",
      "...                 ...               ...  \n",
      "13151          0.033984          0.158203  \n",
      "13152          0.039844          0.151562  \n",
      "13153          0.031929          0.206522  \n",
      "13154          0.034307          0.179008  \n",
      "13155          0.030231          0.197351  \n",
      "\n",
      "[13156 rows x 8 columns] 0         brno_01\n",
      "1         brno_01\n",
      "2         brno_01\n",
      "3         brno_01\n",
      "4         brno_01\n",
      "           ...   \n",
      "13151    smart_99\n",
      "13152    smart_99\n",
      "13153    smart_99\n",
      "13154    smart_99\n",
      "13155    smart_99\n",
      "Name: Participant, Length: 13156, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3092787/17424155.py:81: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_data = data.groupby('Participant').apply(lambda x: x.iloc[:int(len(x)*(1-test_ratio))]).reset_index(drop=True)\n",
      "/tmp/ipykernel_3092787/17424155.py:82: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_data = data.groupby('Participant').apply(lambda x: x.iloc[int(len(x)*(1-test_ratio)):]).reset_index(drop=True)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m y_train_id_cat \u001b[38;5;241m=\u001b[39m to_categorical(y_train_id_encoded)\n\u001b[1;32m     17\u001b[0m y_test_id_cat \u001b[38;5;241m=\u001b[39m to_categorical(y_test_id_encoded)\n\u001b[0;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m([\n\u001b[1;32m     20\u001b[0m     Dense(\u001b[38;5;241m256\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(X_train_id\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],)),\n\u001b[1;32m     21\u001b[0m     Dropout(\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m     22\u001b[0m     Dense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     23\u001b[0m     Dropout(\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m     24\u001b[0m     Dense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     25\u001b[0m     Dropout(\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m     26\u001b[0m     Dense(y_train_id_cat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Multi-class output\u001b[39;00m\n\u001b[1;32m     27\u001b[0m ])\n\u001b[1;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m),\n\u001b[1;32m     30\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Updated loss function\u001b[39;00m\n\u001b[1;32m     31\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     33\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train_id, y_train_id_cat, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Participant ID Identification\n",
    "train_data_id, test_data_id = time_series_split(combined_data, test_ratio=0.2)\n",
    "X_train_id, y_train_id = prepare_features_labels(train_data_id, 'Participant', encode=False)\n",
    "X_test_id, y_test_id = prepare_features_labels(test_data_id, 'Participant', encode=False)\n",
    "print(\"\\n--- Participant ID Identification ---\")\n",
    "\n",
    "print(X_test_id, y_test_id)\n",
    "\n",
    "# Encoding the labels\n",
    "encoder = LabelEncoder()\n",
    "y_train_id_encoded = encoder.fit_transform(y_train_id)\n",
    "y_test_id_encoded = encoder.transform(y_test_id)\n",
    "y_train_id_cat = to_categorical(y_train_id_encoded)\n",
    "y_test_id_cat = to_categorical(y_test_id_encoded)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train_id.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y_train_id_cat.shape[1], activation='softmax')  # Multi-class output\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.05),\n",
    "              loss='categorical_crossentropy',  # Updated loss function\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_id, y_train_id_cat, validation_split=0.2, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "results = model.evaluate(X_test_id, y_test_id_cat, verbose=1)\n",
    "predictions = model.predict(X_test_id)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test_id_cat, axis=1)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "precision = precision_score(true_classes, predicted_classes, average='weighted')\n",
    "recall = recall_score(true_classes, predicted_classes, average='weighted')\n",
    "f1 = f1_score(true_classes, predicted_classes, average='weighted')\n",
    "roc_auc = roc_auc_score(y_test_id_cat, predictions, multi_class='ovr', average='macro')\n",
    "\n",
    "print(\"\\n--- ID Identification Results ---\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"ROC AUC Score:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis\n",
    "explainer = shap.KernelExplainer(model.predict, X_train[:1000])  # Using a subset for kernel approximation\n",
    "shap_values = explainer.shap_values(X_test[:1000])  # Using a subset of test for quick computation\n",
    "\n",
    "# Plot SHAP values\n",
    "shap.summary_plot(shap_values, X_test[:1000], feature_names=X_train.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
