{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/Users/ziyuwang/PycharmProjects/biosignal_anonym/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/records500/00000/00001_hr.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T07:36:51.280687Z",
     "start_time": "2024-03-26T07:36:21.166196Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import ast\n",
    "\n",
    "def load_raw_data(df, sampling_rate, path):\n",
    "    if sampling_rate == 100:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_lr]\n",
    "    else:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_hr]\n",
    "    df = data\n",
    "    data = np.array([signal for signal, meta in data])\n",
    "    return data, df\n",
    "\n",
    "path = './ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/'\n",
    "sampling_rate=100\n",
    "\n",
    "# load and convert annotation data\n",
    "Y = pd.read_csv(path+'ptbxl_database.csv', index_col='ecg_id')\n",
    "Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "X, df = load_raw_data(Y, sampling_rate, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T07:31:55.029796Z",
     "start_time": "2024-03-26T07:31:54.994587Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T07:40:38.959279Z",
     "start_time": "2024-03-26T07:40:38.535761Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T07:48:04.648516Z",
     "start_time": "2024-03-26T07:48:04.640620Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T07:45:15.615830Z",
     "start_time": "2024-03-26T07:45:15.607201Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "signal, fields = wfdb.rdsamp('ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/records500/00000/00001_hr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T07:45:17.055687Z",
     "start_time": "2024-03-26T07:45:17.046021Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ecg_signal = signal[:, 0]  # Assuming the ECG signal is the first column\n",
    "sample_rate = fields['fs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T07:46:07.552419Z",
     "start_time": "2024-03-26T07:46:06.372132Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import neurokit2 as nk\n",
    "import pandas as pd\n",
    "cleaned_ecg = nk.ecg_clean(ecg_signal, sampling_rate=sample_rate)\n",
    "\n",
    "# Find R-peaks\n",
    "_, rpeaks = nk.ecg_peaks(cleaned_ecg, sampling_rate=sample_rate)\n",
    "\n",
    "# Delineate the ECG signal\n",
    "_, waves = nk.ecg_delineate(cleaned_ecg, rpeaks, sampling_rate=sample_rate, method=\"peak\", show_type=\"peaks\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T07:46:13.602215Z",
     "start_time": "2024-03-26T07:46:13.597076Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rpeaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T07:47:28.507729Z",
     "start_time": "2024-03-26T07:47:28.362531Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot = nk.events_plot(rpeaks['ECG_R_Peaks'][:], cleaned_ecg[:], color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T07:58:05.842030Z",
     "start_time": "2024-03-26T07:58:05.692564Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot = nk.events_plot([waves['ECG_T_Peaks'], waves['ECG_P_Peaks'], waves['ECG_Q_Peaks'], waves['ECG_S_Peaks']], cleaned_ecg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T07:58:21.251004Z",
     "start_time": "2024-03-26T07:58:21.175633Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot = nk.events_plot(waves['ECG_P_Peaks'], cleaned_ecg, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def extract_pqrst_features(ecg_signal, sample_rate):\n",
    "    cleaned_ecg = nk.ecg_clean(ecg_signal, sampling_rate=sample_rate)\n",
    "    _, rpeaks = nk.ecg_peaks(cleaned_ecg, sampling_rate=sample_rate)\n",
    "\n",
    "    # Delineate the ECG signal\n",
    "    _, waves_peak = nk.ecg_delineate(cleaned_ecg, rpeaks, sampling_rate=sample_rate, method=\"peak\")\n",
    "\n",
    "    rp_amp_idx = [i for i in range(len(waves_peak['ECG_P_Peaks'])) if\n",
    "                  not math.isnan(rpeaks['ECG_R_Peaks'][i]) and not math.isnan(waves_peak['ECG_P_Peaks'][i])]\n",
    "    rq_amp_idx = [i for i in range(len(waves_peak['ECG_Q_Peaks'])) if\n",
    "                  not math.isnan(rpeaks['ECG_R_Peaks'][i]) and not math.isnan(waves_peak['ECG_Q_Peaks'][i])]\n",
    "    rs_amp_idx = [i for i in range(len(waves_peak['ECG_S_Peaks'])) if\n",
    "                  not math.isnan(rpeaks['ECG_R_Peaks'][i]) and not math.isnan(waves_peak['ECG_S_Peaks'][i])]\n",
    "    rt_amp_idx = [i for i in range(len(waves_peak['ECG_T_Peaks'])) if\n",
    "                  not math.isnan(rpeaks['ECG_R_Peaks'][i]) and not math.isnan(waves_peak['ECG_T_Peaks'][i])]\n",
    "\n",
    "    amp_p = [cleaned_ecg[waves_peak['ECG_P_Peaks'][i]] for i in rp_amp_idx]\n",
    "    amp_rp = [cleaned_ecg[rpeaks['ECG_R_Peaks'][i]] for i in rp_amp_idx]\n",
    "\n",
    "    amp_q = [cleaned_ecg[waves_peak['ECG_Q_Peaks'][i]] for i in rq_amp_idx]\n",
    "    amp_rq = [cleaned_ecg[rpeaks['ECG_R_Peaks'][i]] for i in rq_amp_idx]\n",
    "\n",
    "    amp_s = [cleaned_ecg[waves_peak['ECG_S_Peaks'][i]] for i in rs_amp_idx]\n",
    "    amp_rs = [cleaned_ecg[rpeaks['ECG_R_Peaks'][i]] for i in rs_amp_idx]\n",
    "\n",
    "    amp_t = [cleaned_ecg[waves_peak['ECG_T_Peaks'][i]] for i in rt_amp_idx]\n",
    "    amp_rt = [cleaned_ecg[rpeaks['ECG_R_Peaks'][i]] for i in rt_amp_idx]\n",
    "\n",
    "    rp_amp = np.array([a - b for a, b in zip(amp_rp, amp_p)])\n",
    "    rq_amp = np.array([a - b for a, b in zip(amp_rq, amp_q)])\n",
    "    rs_amp = np.array([a - b for a, b in zip(amp_rs, amp_s)])\n",
    "    rt_amp = np.array([a - b for a, b in zip(amp_rt, amp_t)])\n",
    "\n",
    "    pq_dis = np.array([a - b for a, b in zip(waves_peak['ECG_Q_Peaks'], waves_peak['ECG_P_Peaks'])])\n",
    "    qr_dis = np.array([a - b for a, b in zip(rpeaks['ECG_R_Peaks'], waves_peak['ECG_Q_Peaks'])])\n",
    "    rs_dis = np.array([a - b for a, b in zip(waves_peak['ECG_S_Peaks'], rpeaks['ECG_R_Peaks'])])\n",
    "    st_dis = np.array([a - b for a, b in zip(waves_peak['ECG_T_Peaks'], waves_peak['ECG_S_Peaks'])])\n",
    "\n",
    "    rp_avg = np.mean(rp_amp[~np.isnan(rp_amp)])\n",
    "    rq_avg = np.mean(rq_amp[~np.isnan(rq_amp)])\n",
    "    rs_avg = np.mean(rs_amp[~np.isnan(rs_amp)])\n",
    "    rt_avg = np.mean(rt_amp[~np.isnan(rt_amp)])\n",
    "\n",
    "    pq_avg = np.mean(pq_dis[~np.isnan(pq_dis)])\n",
    "    qr_avg = np.mean(qr_dis[~np.isnan(qr_dis)])\n",
    "    rs_avg = np.mean(rs_dis[~np.isnan(rs_dis)])\n",
    "    st_avg = np.mean(st_dis[~np.isnan(st_dis)])\n",
    "\n",
    "    return [pq_avg, qr_avg, rs_avg, st_avg, rp_avg, rq_avg, rs_avg, rt_avg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wfdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "def load_and_extract_features(metadata_df, directory_path, extract_features_func, max_users=1000):\n",
    "    all_features = []\n",
    "    all_labels = {'patient_id': [], 'age': [], 'gender': [], 'height': [], 'weight': [], 'age_binned': []}\n",
    "    \n",
    "    user_processed = 0\n",
    "    age_bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  # Define bins\n",
    "    bin_labels = ['0-10', '10-20', '20-30', '30-40', '40-50', '50-60', '60-70', '70-80', '80-90', '90-100']  # Labels for the bins\n",
    "    \n",
    "    failed_patients = []\n",
    "    for index, row in metadata_df.iterrows():\n",
    "        if user_processed >= max_users:\n",
    "            break  # Limit the number of users processed\n",
    "        \n",
    "        try:\n",
    "            record_path = os.path.join(directory_path, row['filename_lr'])\n",
    "            print(f\"Processing record {record_path} for patient ID {row['patient_id']}...\")\n",
    "            \n",
    "            signal, fields = wfdb.rdsamp(record_path)\n",
    "            lead_II_signal = signal[:, 1]  # Assuming Lead II is at index 1\n",
    "            features = extract_features_func(lead_II_signal, fields['fs'])\n",
    "            print(f\"Extracted features: {features}\")\n",
    "            \n",
    "            if features is not None:\n",
    "                all_features.append(features)\n",
    "                all_labels['patient_id'].append(row['patient_id'])\n",
    "                all_labels['age'].append(row['age'])\n",
    "                all_labels['gender'].append(row['sex'])\n",
    "                all_labels['height'].append(row['height'])\n",
    "                all_labels['weight'].append(row['weight'])\n",
    "\n",
    "                # Perform age binning for the current patient and append\n",
    "                current_age_bin = pd.cut([row['age']], bins=age_bins, labels=bin_labels, right=False)[0]\n",
    "                all_labels['age_binned'].append(current_age_bin)\n",
    "                \n",
    "                user_processed += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process record {record_path}: {e}\")\n",
    "            failed_patients.append(row['patient_id'])\n",
    "            \n",
    "    \n",
    "    print(f\"Processed {user_processed} users.\")\n",
    "    return np.array(all_features), all_labels, failed_patients\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(X_train, y_train, model, task_name, is_classification=True, X_test=None, y_test=None):\n",
    "    # Check if X_test and y_test are provided, otherwise split X_train and y_train\n",
    "    if X_test is None or y_test is None:\n",
    "        cutoff = int(len(X_train) * 0.8)\n",
    "        X_train, X_test = X_train[:cutoff], X_train[cutoff:]\n",
    "        y_train, y_test = y_train[:cutoff], y_train[cutoff:]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    if is_classification:\n",
    "        print(f\"Classification report for {task_name}:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        # judge if it is a binary classification\n",
    "        if len(np.unique(y_test)) == 2:\n",
    "            precision = precision_score(y_test, y_pred, average='binary')\n",
    "            recall = recall_score(y_test, y_pred, average='binary')\n",
    "            f1 = f1_score(y_test, y_pred, average='binary')\n",
    "        else:\n",
    "            precision = precision_score(y_test, y_pred, average='weighted')  # Use 'binary' for binary classification, 'weighted' for multi-class\n",
    "            recall = recall_score(y_test, y_pred, average='weighted')\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "        print(f\"Precision: {precision:.2f}\")\n",
    "        print(f\"Recall: {recall:.2f}\")\n",
    "        print(f\"F1-score: {f1:.2f}\")\n",
    "    else:\n",
    "        print(f\"Mean Absolute Error for {task_name}: {mean_absolute_error(y_test, y_pred)}\")\n",
    "        \n",
    "        print(f\"R-squared for {task_name}: {r2_score(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "\n",
    "def print_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Define your models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=500),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=500),\n",
    "    'Baysesian': GaussianNB()\n",
    "}\n",
    "\n",
    "path_to_dataset = './ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/'\n",
    "metadata_df = pd.read_csv(os.path.join(path_to_dataset, 'ptbxl_database.csv'))\n",
    "metadata_df['filename_lr'] = metadata_df['filename_lr'].apply(lambda x: x.replace('.mat', ''))\n",
    "\n",
    "max_users = 21837  # Make the number of users adjustable\n",
    "train_ratio = 0.8  # Adjustable train-test split ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender prediction with patient ID split\n",
    "\n",
    "# Clean the DataFrame based on 'sex' column and limit the data to 'max_users'\n",
    "clean_metadata_df_gender = metadata_df.dropna(subset=['sex']).head(max_users)\n",
    "\n",
    "# Load and extract features\n",
    "features_gender, labels_gender, failed_patients = load_and_extract_features(clean_metadata_df_gender, path_to_dataset, extract_pqrst_features, max_users)\n",
    "\n",
    "# Report on cleaned data\n",
    "original_count = len(metadata_df)\n",
    "clean_count = len(clean_metadata_df_gender) - len(failed_patients)  # Adjusted for failed patients\n",
    "print(f\"Original user count: {original_count}\")\n",
    "print(f\"Clean user count (no NaNs in sex, excluding failed patients): {clean_count}\")\n",
    "print(f\"Ratio of clean data: {clean_count / original_count:.2f}\")\n",
    "\n",
    "# Encode gender labels\n",
    "le = LabelEncoder()\n",
    "y_gender = le.fit_transform(labels_gender['gender'])\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled_gender = scaler.fit_transform(features_gender)\n",
    "\n",
    "unique_patient_ids = clean_metadata_df_gender['patient_id'].unique()\n",
    "split_index = int(len(unique_patient_ids) * 0.8)\n",
    "\n",
    "train_patient_ids = unique_patient_ids[:split_index]\n",
    "test_patient_ids = unique_patient_ids[split_index:]\n",
    "\n",
    "# Distinguished by patient_id: Perform an 80-20 split based on unique patient IDs\n",
    "train_patient_ids = set(train_patient_ids) - set(failed_patients)  # Exclude failed patients\n",
    "test_patient_ids = set(test_patient_ids) - set(failed_patients)  # Exclude failed patients\n",
    "\n",
    "# Filter the dataset for training and testing, excluding failed patients\n",
    "train_mask = np.isin(labels_gender['patient_id'], list(train_patient_ids))\n",
    "test_mask = np.isin(labels_gender['patient_id'], list(test_patient_ids))\n",
    "\n",
    "X_train_gender = X_scaled_gender[train_mask]\n",
    "y_train_gender = y_gender[train_mask]\n",
    "\n",
    "X_test_gender = X_scaled_gender[test_mask]\n",
    "y_test_gender = y_gender[test_mask]\n",
    "\n",
    "print(\"Predicting Gender:\")\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nUsing model: {model_name}\")\n",
    "    model.fit(X_train_gender, y_train_gender)\n",
    "    y_pred = model.predict(X_test_gender)\n",
    "    # Assuming train_and_evaluate is defined and properly handles evaluation\n",
    "    train_and_evaluate(X_train_gender, y_train_gender, model, 'Gender', is_classification=True, X_test=X_test_gender, y_test=y_test_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "print(\"Predicting Gender:\")\n",
    "# Fit the imputer on the training data and transform it\n",
    "X_train_gender_imputed = imputer.fit_transform(X_train_gender)\n",
    "\n",
    "# Transform the test data using the same imputer\n",
    "X_test_gender_imputed = imputer.transform(X_test_gender)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nUsing model: {model_name}\")\n",
    "    model.fit(X_train_gender_imputed, y_train_gender)\n",
    "    y_pred = model.predict(X_test_gender_imputed)\n",
    "    # Assuming train_and_evaluate is defined and properly handles evaluation\n",
    "    train_and_evaluate(X_train_gender_imputed, y_train_gender, model, 'Gender', is_classification=True, X_test=X_test_gender_imputed, y_test=y_test_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age prediction with patient ID split\n",
    "\n",
    "max_users = 21837  # Make the number of users adjustable\n",
    "\n",
    "# Clean the DataFrame based on 'age' column and limit the data to 'max_users'\n",
    "clean_metadata_df_age = metadata_df.dropna(subset=['age']).head(max_users)\n",
    "\n",
    "# Load and extract features, adjusting the function to handle age\n",
    "features_age, labels_age, failed_patients_age = load_and_extract_features(clean_metadata_df_age, path_to_dataset, extract_pqrst_features, max_users)\n",
    "\n",
    "# Convert age to categorical bins for classification\n",
    "# age_bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "age_bins = [0, 20, 40, 60, 80, 100]\n",
    "labels_age['age_binned'] = pd.cut(labels_age['age'], bins=age_bins, labels=False)  # Using integer labels for bins\n",
    "\n",
    "# Encode binned age labels\n",
    "le_age = LabelEncoder()\n",
    "y_age_binned = le_age.fit_transform(labels_age['age_binned'])\n",
    "\n",
    "# Scale features\n",
    "scaler_age = StandardScaler()\n",
    "X_scaled_age = scaler_age.fit_transform(features_age)\n",
    "\n",
    "# Perform a patient ID split, adjusted for age analysis\n",
    "train_patient_ids_age = set(clean_metadata_df_age['patient_id'][:split_index].unique()) - set(failed_patients_age)\n",
    "test_patient_ids_age = set(clean_metadata_df_age['patient_id'][split_index:].unique()) - set(failed_patients_age)\n",
    "\n",
    "train_mask_age = np.isin(labels_age['patient_id'], list(train_patient_ids_age))\n",
    "test_mask_age = np.isin(labels_age['patient_id'], list(test_patient_ids_age))\n",
    "\n",
    "X_train_age_binned = X_scaled_age[train_mask_age]\n",
    "y_train_age_binned = y_age_binned[train_mask_age]\n",
    "\n",
    "X_test_age_binned = X_scaled_age[test_mask_age]\n",
    "y_test_age_binned = y_age_binned[test_mask_age]\n",
    "\n",
    "print(\"Predicting Age Bin:\")\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nUsing model: {model_name}\")\n",
    "    model.fit(X_train_age_binned, y_train_age_binned)\n",
    "    y_pred = model.predict(X_test_age_binned)\n",
    "    # Assuming train_and_evaluate is defined and properly handles evaluation\n",
    "    train_and_evaluate(X_train_age_binned, y_train_age_binned, model, 'Age Bin', is_classification=True, X_test=X_test_age_binned, y_test=y_test_age_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "print(\"Predicting Age Bin:\")\n",
    "# Fit the imputer on the training data and transform it\n",
    "\n",
    "X_train_age_binned = X_scaled_age[train_mask_age]\n",
    "y_train_age_binned = y_age_binned[train_mask_age]\n",
    "\n",
    "X_test_age_binned = X_scaled_age[test_mask_age]\n",
    "y_test_age_binned = y_age_binned[test_mask_age]\n",
    "\n",
    "X_train_age_binned_imputed = imputer.fit_transform(X_train_age_binned)\n",
    "\n",
    "# Transform the test data using the same imputer\n",
    "X_test_age_binned_imputed = imputer.transform(X_test_age_binned)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nUsing model: {model_name}\")\n",
    "    model.fit(X_train_age_binned_imputed, y_train_age_binned)\n",
    "    y_pred = model.predict(X_test_age_binned_imputed)\n",
    "    # Assuming train_and_evaluate is defined and properly handles evaluation\n",
    "    train_and_evaluate(X_train_age_binned_imputed, y_train_age_binned, model, 'Age Bin', is_classification=True, X_test=X_test_age_binned_imputed, y_test=y_test_age_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming metadata_df, path_to_dataset, and extract_pqrst_features are predefined\n",
    "# Adjust max_users to control the number of patients processed\n",
    "max_users = 21837\n",
    "\n",
    "# Load and extract features for patient reidentification\n",
    "features, labels, failed_patients = load_and_extract_features(metadata_df, path_to_dataset, extract_pqrst_features, max_users)\n",
    "\n",
    "# Assuming 'patient_id' is included in labels and corresponds to each row in features\n",
    "patient_ids = np.array(labels['patient_id'])\n",
    "\n",
    "# Remove data related to failed patients\n",
    "success_mask = ~np.isin(patient_ids, failed_patients)\n",
    "features_clean = features[success_mask]\n",
    "patient_ids_clean = patient_ids[success_mask]\n",
    "\n",
    "# Encode patient IDs as numeric labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(patient_ids_clean)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features_clean)\n",
    "\n",
    "# Splitting data: Instead of a random train-test split, do a stratified split based on patient IDs\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nUsing model: {model_name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Assuming train_and_evaluate is defined and properly handles evaluation\n",
    "    train_and_evaluate(X_train, y_train, model, 'Patient ID', is_classification=True, X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "print(\"Predicting ID:\")\n",
    "# Fit the imputer on the training data and transform it\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same imputer\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nUsing model: {model_name}\")\n",
    "    model.fit(X_train_imputed, y_train)\n",
    "    y_pred = model.predict(X_test_imputed)\n",
    "    # Assuming train_and_evaluate is defined and properly handles evaluation\n",
    "    train_and_evaluate(X_train_imputed, y_train, model, 'Patient ID', is_classification=True, X_test=X_test_imputed, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming metadata_df, path_to_dataset, and extract_pqrst_features are predefined\n",
    "# Adjust max_users to control the number of patients processed\n",
    "max_users = 100\n",
    "\n",
    "# Load and extract features for patient reidentification\n",
    "features, labels, failed_patients = load_and_extract_features(metadata_df, path_to_dataset, extract_pqrst_features, max_users)\n",
    "\n",
    "\n",
    "# Assuming 'patient_id' is included in labels and corresponds to each row in features\n",
    "patient_ids = np.array(labels['patient_id'])\n",
    "\n",
    "# Remove data related to failed patients\n",
    "success_mask = ~np.isin(patient_ids, failed_patients)\n",
    "features_clean = features[success_mask]\n",
    "patient_ids_clean = patient_ids[success_mask]\n",
    "\n",
    "# Encode patient IDs as numeric labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(patient_ids_clean)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features_clean)\n",
    "\n",
    "# Splitting data: Instead of a random train-test split, do a stratified split based on patient IDs\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "print(\"Predicting ID:\")\n",
    "# Fit the imputer on the training data and transform it\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same imputer\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "print(y_test)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nUsing model: {model_name}\")\n",
    "    model.fit(X_train_imputed, y_train)\n",
    "    y_pred = model.predict(X_test_imputed)\n",
    "    # Assuming train_and_evaluate is defined and properly handles evaluation\n",
    "    train_and_evaluate(X_train_imputed, y_train, model, 'Patient ID', is_classification=True, X_test=X_test_imputed, y_test=y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
