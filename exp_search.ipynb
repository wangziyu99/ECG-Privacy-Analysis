{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "def load_data(dataset_folders):\n",
    "    combined_data = []\n",
    "    for folder in dataset_folders:  # Update with the path to your datasets\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                filepath = os.path.join(folder, filename)\n",
    "                # print(f\"Loading {filepath}\")\n",
    "                participant_data = pd.read_csv(filepath)\n",
    "                combined_data.append(participant_data)\n",
    "    return pd.concat(combined_data, ignore_index=True)\n",
    "\n",
    "param_grid_lr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'lbfgs'],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "param_grid_dt = {\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "    'class_weight': [None, 'balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.001],\n",
    "    # 'max_depth': [3, 6, 9],\n",
    "    # 'subsample': [0.5, 0.7, 1.0],\n",
    "    # 'colsample_bytree': [0.5, 0.7, 1.0],\n",
    "    # 'gamma': [0, 0.1, 0.2],\n",
    "    # 'reg_lambda': [0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "def perform_grid_search(estimator, param_grid, X, y, scoring='accuracy', cv=5):\n",
    "    grid_search = GridSearchCV(estimator, param_grid, scoring=scoring, cv=cv, verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X, y)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best score:\", grid_search.best_score_)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def train_test_split_by_participant(data, test_size=0.2):\n",
    "    \"\"\"Split data by participant for age and gender models.\"\"\"\n",
    "    participants = data['Participant'].unique()\n",
    "    np.random.shuffle(participants)\n",
    "    test_size = int(len(participants) * test_size)\n",
    "    test_participants = participants[-test_size:]\n",
    "    train_participants = participants[:-test_size]\n",
    "\n",
    "    train_data = data[data['Participant'].isin(train_participants)]\n",
    "    test_data = data[data['Participant'].isin(test_participants)]\n",
    "\n",
    "    # Detailed split information\n",
    "    print(\"Training on participants from databases:\\n\", train_data[['Participant', 'Database']].drop_duplicates())\n",
    "    print(\"Testing on participants from databases:\\n\", test_data[['Participant', 'Database']].drop_duplicates())\n",
    "    return train_data, test_data\n",
    "\n",
    "def time_series_split(data, test_ratio=0.2):\n",
    "    \"\"\"Time-series split for participant ID identification.\"\"\"\n",
    "    train_data = data.groupby('Participant').apply(lambda x: x.iloc[:int(len(x)*(1-test_ratio))]).reset_index(drop=True)\n",
    "    test_data = data.groupby('Participant').apply(lambda x: x.iloc[int(len(x)*(1-test_ratio)):]).reset_index(drop=True)\n",
    "    \n",
    "    # Detailed split information\n",
    "    for participant, group in data.groupby('Participant'):\n",
    "        train_end = int(len(group)*(1-test_ratio))\n",
    "        # print(f\"Participant {participant} from {group['Database'].iloc[0]}: Train indices 0 to {train_end}, Test indices {train_end} to {len(group)}\")\n",
    "    return train_data, test_data\n",
    "\n",
    "def prepare_features_labels(data, label_column, encode=True):\n",
    "    \"\"\"Prepare features and labels for training.\"\"\"\n",
    "    features = data.drop(columns=['Participant', 'Sample', 'Sampling_Rate', 'Database', 'Gender', 'Age', 'age_binned'])\n",
    "    labels = data[label_column]\n",
    "    if encode:\n",
    "        encoder = LabelEncoder()\n",
    "        labels = encoder.fit_transform(labels)\n",
    "    return features, labels\n",
    "\n",
    "def run_shap_analysis(model, X_train, X_test, model_name, task, aggregate_classes=False):\n",
    "    # Sample a subset of data if X_test is too large\n",
    "    X_test_sample = X_train.sample(n=1000, random_state=42) if len(X_test) > 1000 else X_test\n",
    "    \n",
    "    # Choose the right explainer based on model type\n",
    "    if model_name == 'Logistic Regression':\n",
    "        explainer = shap.LinearExplainer(model=model, data=X_train, masker=shap.maskers.Independent(data=X_train))\n",
    "        shap_values = explainer.shap_values(X_test_sample)\n",
    "    elif model_name in ['Decision Tree', 'Random Forest', 'XGBoost']:\n",
    "        explainer = shap.TreeExplainer(model=model, data=X_train)\n",
    "        shap_values = explainer.shap_values(X_test_sample, check_additivity=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type {model_name}\")\n",
    "    \n",
    "    \n",
    "    feature_names = X_test_sample.columns.tolist()\n",
    "\n",
    "    # Handle multi-class outputs\n",
    "    if len(shap_values.shape) == 3:\n",
    "        if aggregate_classes:\n",
    "            # Aggregate SHAP values across classes\n",
    "            aggregated_shap_values = np.mean(np.abs(shap_values), axis=2)\n",
    "            aggregated_shap_values = np.mean(shap_values, axis=2)\n",
    "            plot_shap_summary(aggregated_shap_values, X_test_sample, model_name, task, feature_names=feature_names)\n",
    "        else:\n",
    "            # Visualize each class separately\n",
    "            for i in range(shap_values.shape[2]):\n",
    "                plot_shap_summary(shap_values[:, :, i], X_test_sample, model_name, task, class_index=i, feature_names=feature_names)\n",
    "                # plot_decision_plot(explainer.expected_value[i], shap_values[:, :, i], feature_names, model_name, task, class_index=i)\n",
    "    else:\n",
    "        plot_shap_summary(shap_values, X_test_sample, model_name, task, feature_names=feature_names)\n",
    "        # plot_decision_plot(explainer.expected_value, shap_values, feature_names, model_name, task)\n",
    "\n",
    "def plot_shap_summary(shap_values, features, model_name, task, class_index=None, feature_names=None):\n",
    "    suffix = f\"_class_{class_index}\" if class_index is not None else \"\"\n",
    "    plt.figure(figsize=(40, 12))\n",
    "    shap.summary_plot(shap_values, features=features, feature_names=feature_names, plot_type=\"bar\", show=False)\n",
    "    plt.title(f'SHAP Summary Plot (Bar) - {model_name} for {task}{suffix}')\n",
    "    plt.savefig(f'plot/{model_name}_{task}{suffix}_shap_summary_bar.pdf')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(40, 12))\n",
    "    shap.summary_plot(shap_values, features=features, feature_names=feature_names, show=False)\n",
    "    plt.title(f'SHAP Detailed Summary Plot - {model_name} for {task}{suffix}')\n",
    "    plt.savefig(f'plot/{model_name}_{task}{suffix}_shap_detailed.pdf')\n",
    "    plt.close()\n",
    "\n",
    "def plot_decision_plot(base_value, shap_values, feature_names, model_name, task, class_index=None):\n",
    "    suffix = f\"_class_{class_index}\" if class_index is not None else \"\"\n",
    "    plt.figure(figsize=(40, 12))\n",
    "    shap.decision_plot(base_value, shap_values, feature_names=feature_names, show=False)\n",
    "    plt.title(f'Decision Plot - {model_name} for {task}{suffix} (Sample Instances)')\n",
    "    plt.savefig(f'plot/{model_name}_{task}{suffix}_decision_plot.pdf')\n",
    "    plt.close()\n",
    "\n",
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, n=1):\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "    best_predictions = None\n",
    "    best_probabilities = None\n",
    "    best_roc_auc = None\n",
    "    for _ in range(n):\n",
    "        cloned_model = clone(model)\n",
    "        cloned_model.fit(X_train, y_train)\n",
    "        predictions = cloned_model.predict(X_test)\n",
    "        probabilities = cloned_model.predict_proba(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        # try:\n",
    "        #     if probabilities.shape[1] == 2:\n",
    "        #         roc_auc = roc_auc_score(y_test, probabilities[:, 1])\n",
    "        #     else:\n",
    "        #         # Binarize the output\n",
    "        #         y_test_bin = label_binarize(y_test, classes=classes)\n",
    "        #         roc_auc = roc_auc_score(y_test_bin, probabilities, multi_class='ovr')\n",
    "        # except Exception as e:\n",
    "        #     roc_auc = None\n",
    "        #     print(f\"ROC AUC could not be calculated: {e}\")\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = cloned_model\n",
    "            best_predictions = predictions\n",
    "            best_probabilities = probabilities\n",
    "            \n",
    "    return best_model, best_accuracy, best_predictions, best_probabilities\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name, task):\n",
    "    # model.fit(X_train, y_train)\n",
    "    \n",
    "    model, best_accuracy, predictions, probabilities = train_and_evaluate(model, X_train, y_train, X_test, y_test)\n",
    "    print(f\"Selected best model with accuracy: {best_accuracy:.4f}\")\n",
    "    # predictions = model.predict(X_test)\n",
    "    # probabilities = model.predict_proba(X_test)\n",
    "    classes = np.unique(np.concatenate([y_train, y_test]))\n",
    "\n",
    "    # print(f\"Shapes - Predictions: {predictions.shape}, Probabilities: {probabilities.shape}\")\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='weighted')\n",
    "    recall = recall_score(y_test, predictions, average='weighted')\n",
    "    f1 = f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "    try:\n",
    "        if probabilities.shape[1] == 2:\n",
    "            roc_auc = roc_auc_score(y_test, probabilities[:, 1])\n",
    "        else:\n",
    "            # Binarize the output\n",
    "            y_test_bin = label_binarize(y_test, classes=classes)\n",
    "            roc_auc = roc_auc_score(y_test_bin, probabilities, multi_class='ovr')\n",
    "    except Exception as e:\n",
    "        roc_auc = None\n",
    "        print(f\"ROC AUC could not be calculated: {e}\")\n",
    "\n",
    "    print(\"Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    if roc_auc:\n",
    "        print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    if task == 'Participant ID':\n",
    "        plt.figure(figsize=(72, 54))\n",
    "    else: \n",
    "        plt.figure(figsize=(12, 9))\n",
    "        \n",
    "    threshold = np.percentile(cm, 0)  # Adjust the percentile threshold\n",
    "    mask = cm < threshold\n",
    "    masked_confusion_matrix = np.where(mask, np.nan, cm)  \n",
    "    sns.heatmap(masked_confusion_matrix, annot=True, fmt='.2f', cmap='Blues', xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))\n",
    "    plt.title(f'Confusion Matrix for {model_name} on {task}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(f'plot/Confusion_Matrix_{model_name}_{task}.pdf')  # Save the confusion matrix figure\n",
    "    plt.show()\n",
    "\n",
    "    if probabilities.shape[1] == 2:  # Only plot ROC for binary classification\n",
    "        fpr, tpr, _ = roc_curve(y_test, probabilities[:, 1])\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label='ROC Curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], 'r--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve for {model_name} on {task}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(f'plot/{model_name}_{task}_ROC_Curve.pdf')  # Save the confusion matrix figure\n",
    "        plt.show()\n",
    "        \n",
    "    # SHAP analysis\n",
    "    try:\n",
    "        # run_shap_analysis(model, X_train, X_test, model_name, task)\n",
    "        run_shap_analysis(model, X_train, X_test, model_name, task, aggregate_classes=True)\n",
    "    except Exception as e:\n",
    "        print(f\"SHAP analysis failed on {task} with model {model_name}\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main experiment setup\n",
    "dataset_paths = ['mit-bih_features/', 'mit-long_features/', 'smart_features/', 'chfdb_features/', 'brno_features/']\n",
    "combined_data = load_data(dataset_paths)\n",
    "# drop the 'Half Minute' column\n",
    "combined_data = combined_data.drop(columns=['Half Minute'])\n",
    "combined_data['Gender'] = combined_data['Gender'].replace({'m': 'M'})\n",
    "\n",
    "combined_data = combined_data.dropna()\n",
    "unique_genders = combined_data['Gender'].unique()\n",
    "\n",
    "age_bins = [0, 18, 35, 50, 65, 80, 100]\n",
    "age_labels = [f\"{age_bins[i]}-{age_bins[i+1]}\" for i in range(len(age_bins)-1)]\n",
    "\n",
    "combined_data['age_binned'] = pd.cut(combined_data['Age'], bins=age_bins, labels=age_labels, right=False)\n",
    "le = LabelEncoder()\n",
    "combined_data['age_binned'] = le.fit_transform(combined_data['age_binned'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data for age and gender identification\n",
    "train_data_ag, test_data_ag = train_test_split_by_participant(combined_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, n_jobs=-1),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(n_jobs=-1),\n",
    "    'XGBoost': GradientBoostingClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender Identification\n",
    "X_train, y_train = prepare_features_labels(train_data_ag, 'Gender')\n",
    "X_test, y_test = prepare_features_labels(test_data_ag, 'Gender')\n",
    "print(\"\\n--- Gender Identification ---\")\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    evaluate_model(model, X_train, y_train, X_test, y_test, model_name=name, task='Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age Identification (Binning ages as needed)\n",
    "X_train_age, y_train_age = prepare_features_labels(train_data_ag, 'age_binned')\n",
    "X_test_age, y_test_age = prepare_features_labels(test_data_ag, 'age_binned')\n",
    "print(\"\\n--- Age Identification ---\")\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    evaluate_model(model, X_train_age, y_train_age, X_test_age, y_test_age, model_name=name, task='Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_classes = combined_data['age_binned'].unique()\n",
    "age_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Participant ID Identification\n",
    "train_data_id, test_data_id = time_series_split(combined_data, test_ratio=0.2)\n",
    "X_train_id, y_train_id = prepare_features_labels(train_data_id, 'Participant', encode=False)\n",
    "X_test_id, y_test_id = prepare_features_labels(test_data_id, 'Participant', encode=False)\n",
    "print(\"\\n--- Participant ID Identification ---\")\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    evaluate_model(model, X_train_id, y_train_id, X_test_id, y_test_id, model_name=name, task='Participant ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert IDs to strings\n",
    "y_train_id = y_train_id.astype(str)\n",
    "\n",
    "# Now check the unique values\n",
    "print(\"Unique values in 'y_train_id':\", np.unique(y_train_id))\n",
    "\n",
    "# Check for NaN values after conversion\n",
    "print(\"Number of NaN values in 'y_train_id':\", y_train_id.isnull().sum())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
