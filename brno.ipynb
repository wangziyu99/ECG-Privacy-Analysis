{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for Participant ID: 118001\n",
      "[[32767.65535311]\n",
      " [32767.65535311]\n",
      " [32767.65535311]\n",
      " ...\n",
      " [  832.01664033]\n",
      " [  853.01706034]\n",
      " [  903.01806036]]\n",
      "Processing data for Participant ID: 100002\n",
      "[[32768.11501187]\n",
      " [32768.11501187]\n",
      " [32768.11501187]\n",
      " ...\n",
      " [-1790.00064181]\n",
      " [-1777.16449522]\n",
      " [-1777.16449522]]\n",
      "Processing data for Participant ID: 126001\n",
      "[[213.00426009]\n",
      " [199.00398008]\n",
      " [197.00394008]\n",
      " ...\n",
      " [  0.        ]\n",
      " [ 15.00030001]\n",
      " [ 34.00068001]]\n",
      "Processing data for Participant ID: 122001\n",
      "[[32767.65535311]\n",
      " [32767.65535311]\n",
      " [32767.65535311]\n",
      " ...\n",
      " [ -301.00602012]\n",
      " [ -266.00532011]\n",
      " [ -334.00668013]]\n",
      "Processing data for Participant ID: 123001\n",
      "[[ 3861.86853577]\n",
      " [ 3943.07205744]\n",
      " [ 4024.27557911]\n",
      " ...\n",
      " [-2387.38353705]\n",
      " [-2389.94785879]\n",
      " [-2393.36695444]]\n",
      "Processing data for Participant ID: 103001\n",
      "[[19295.42797366]\n",
      " [19077.59522491]\n",
      " [18874.5307981 ]\n",
      " ...\n",
      " [   65.22675528]\n",
      " [   76.91834349]\n",
      " [  102.14756015]]\n",
      "Processing data for Participant ID: 113001\n",
      "[[6016.12032241]\n",
      " [6087.12174243]\n",
      " [6172.12344247]\n",
      " ...\n",
      " [1394.02788056]\n",
      " [1379.02758055]\n",
      " [1366.02732055]]\n",
      "Processing data for Participant ID: 104001\n",
      "[[16831.33662673]\n",
      " [16821.33642673]\n",
      " [16802.33604672]\n",
      " ...\n",
      " [ -865.01730035]\n",
      " [ -854.01708034]\n",
      " [ -832.01664033]]\n",
      "Processing data for Participant ID: 114001\n",
      "[[ 1738.0347607 ]\n",
      " [ 1725.03450069]\n",
      " [ 1733.03466069]\n",
      " ...\n",
      " [-3847.07694154]\n",
      " [-3667.07334147]\n",
      " [-3347.06694134]]\n",
      "Processing data for Participant ID: 100001\n",
      "[[ 2.22905812e+04]\n",
      " [ 2.22795591e+04]\n",
      " [ 2.22805611e+04]\n",
      " ...\n",
      " [-1.30260521e+01]\n",
      " [-2.30460922e+01]\n",
      " [-2.30460922e+01]]\n",
      "Processing data for Participant ID: 124001\n",
      "[[  954.21543932]\n",
      " [  971.25500074]\n",
      " [  985.33116017]\n",
      " ...\n",
      " [-1351.31130538]\n",
      " [-1354.27470736]\n",
      " [-1346.8662024 ]]\n",
      "Processing data for Participant ID: 111001\n",
      "[[21753.4350687 ]\n",
      " [21748.4349687 ]\n",
      " [21770.43540871]\n",
      " ...\n",
      " [   90.00180004]\n",
      " [   68.00136003]\n",
      " [   90.00180004]]\n",
      "Processing data for Participant ID: 125001\n",
      "[[ 3036.06072121]\n",
      " [ 2998.0599612 ]\n",
      " [ 2936.05872117]\n",
      " ...\n",
      " [-2081.04162083]\n",
      " [-2073.04146083]\n",
      " [-2065.04130083]]\n",
      "Processing data for Participant ID: 105001\n",
      "[[ 39.00078002]\n",
      " [-22.00044001]\n",
      " [187.00374007]\n",
      " ...\n",
      " [161.00322006]\n",
      " [214.00428009]\n",
      " [ 60.00120002]]\n",
      "Processing data for Participant ID: 121001\n",
      "[[ 755.0151003 ]\n",
      " [ 759.0151803 ]\n",
      " [ 763.01526031]\n",
      " ...\n",
      " [-221.00442009]\n",
      " [-222.00444009]\n",
      " [-231.00462009]]\n",
      "Processing data for Participant ID: 115001\n",
      "[[3344.06688134]\n",
      " [3334.06668133]\n",
      " [3337.06674133]\n",
      " ...\n",
      " [1694.03388068]\n",
      " [1253.0250605 ]\n",
      " [1035.02070041]]\n",
      "Processing data for Participant ID: 103003\n",
      "[[32766.30308656]\n",
      " [32766.30308656]\n",
      " [32766.30308656]\n",
      " ...\n",
      " [ -176.37502901]\n",
      " [ -249.09104974]\n",
      " [ -103.65900828]]\n",
      "Processing data for Participant ID: 103002\n",
      "[[27293.13206354]\n",
      " [27252.8254169 ]\n",
      " [27217.26072868]\n",
      " ...\n",
      " [ 1687.34687426]\n",
      " [ 1654.15316526]\n",
      " [ 1678.65328381]]\n",
      "All ECG data and annotations have been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "\n",
    "# Define the base path where the dataset is located\n",
    "base_path = \"brno-university-of-technology-ecg-quality-database-but-qdb-1.0.0\"\n",
    "\n",
    "# Gather all .dat files from the subdirectories under the base path\n",
    "data_files = []\n",
    "for root, dirs, files in os.walk(base_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\"_ECG.dat\"):\n",
    "            data_files.append(os.path.join(root, file))\n",
    "\n",
    "dfs_ecg = []\n",
    "dfs_rpeaks = []\n",
    "\n",
    "info = pd.read_csv(\"brno-university-of-technology-ecg-quality-database-but-qdb-1.0.0/subject-info.csv\", sep=';')\n",
    "\n",
    "# Process each data file\n",
    "for participant, file in enumerate(data_files):\n",
    "    print(\"Processing data for Participant ID:\", file.split('/')[-2])\n",
    "   \n",
    "\n",
    "    # Get signal\n",
    "    record = wfdb.rdsamp(file[:-4])  # Remove '.dat' and pass the path without the extension\n",
    "    \n",
    "    age = info[info['ID'] == int(file.split('/')[-2])]['Age'].values[0]\n",
    "    gender = info[info['ID'] == int(file.split('/')[-2])]['Gender'].values[0]\n",
    "    \n",
    "    print(record[0])\n",
    "    data = pd.DataFrame({\"ECG\": record[0][:, 0]})  # Adjust index if needed based on lead configuration\n",
    "    \n",
    "    # Metadata\n",
    "    data[\"Participant\"] = \"brno_%.2i\" %(participant)  # Participant ID from the folder name\n",
    "    data[\"Sample\"] = range(len(data))\n",
    "    data[\"Sampling_Rate\"] = record[1]['fs']  # Sampling rate from the record metadata\n",
    "    data[\"Database\"] = \"brno\"\n",
    "    data = data[:record[1]['fs']*60*120].reset_index(drop=True)\n",
    "    data['Age'] = age\n",
    "    data['Gender'] = gender\n",
    "\n",
    "    # # Getting annotations\n",
    "    # try:\n",
    "    #     anno = wfdb.rdann(file[:-4], 'atr')  # Use correct annotation extension if different\n",
    "    #     valid_rpeaks = anno.sample[np.isin(anno.symbol, [\"N\"])]  # Assuming 'N' denotes normal beats\n",
    "    #     anno_df = pd.DataFrame({\"Rpeaks\": valid_rpeaks})\n",
    "    #     anno_df[\"Participant\"] = file.split('/')[-2]\n",
    "    #     anno_df[\"Sampling_Rate\"] = record[1]['fs']\n",
    "    #     anno_df[\"Database\"] = \"BUT QDB\"\n",
    "    # except FileNotFoundError:\n",
    "    #     print(\"Annotation file not found for\", file.split('/')[-2])\n",
    "    #     continue\n",
    "    \n",
    "    dfs_ecg.append(data)\n",
    "    # dfs_rpeaks.append(anno_df)\n",
    "\n",
    "# Save the concatenated DataFrame of all ECGs and R-peaks to CSV files\n",
    "pd.concat(dfs_ecg).to_csv(\"brno_ECGs.csv\", index=False)\n",
    "# pd.concat(dfs_rpeaks).to_csv(\"BUT_QDB_Rpeaks.csv\", index=False)\n",
    "\n",
    "print(\"All ECG data and annotations have been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 18 participants.\n",
      "Processing data for Participant: brno_00\n",
      "Error processing data for Participant: brno_00\n",
      "'[20016] not in index'\n",
      "Processing data for Participant: brno_01\n",
      "Extracted features saved to 'brno_features/brno_features_brno_01.csv'.\n",
      "Processing data for Participant: brno_02\n",
      "Extracted features saved to 'brno_features/brno_features_brno_02.csv'.\n",
      "Processing data for Participant: brno_03\n",
      "Error processing data for Participant: brno_03\n",
      "'[20032] not in index'\n",
      "Processing data for Participant: brno_04\n",
      "Extracted features saved to 'brno_features/brno_features_brno_04.csv'.\n",
      "Processing data for Participant: brno_05\n",
      "Extracted features saved to 'brno_features/brno_features_brno_05.csv'.\n",
      "Processing data for Participant: brno_06\n",
      "Extracted features saved to 'brno_features/brno_features_brno_06.csv'.\n",
      "Processing data for Participant: brno_07\n",
      "Extracted features saved to 'brno_features/brno_features_brno_07.csv'.\n",
      "Processing data for Participant: brno_08\n",
      "Extracted features saved to 'brno_features/brno_features_brno_08.csv'.\n",
      "Processing data for Participant: brno_09\n",
      "Extracted features saved to 'brno_features/brno_features_brno_09.csv'.\n",
      "Processing data for Participant: brno_10\n",
      "Extracted features saved to 'brno_features/brno_features_brno_10.csv'.\n",
      "Processing data for Participant: brno_11\n",
      "Extracted features saved to 'brno_features/brno_features_brno_11.csv'.\n",
      "Processing data for Participant: brno_12\n",
      "Extracted features saved to 'brno_features/brno_features_brno_12.csv'.\n",
      "Processing data for Participant: brno_13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ziyuwang/anaconda3/envs/ecg/lib/python3.12/site-packages/neurokit2/signal/signal_period.py:84: NeuroKitWarning: Too few peaks detected to compute the rate. Returning empty vector.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing data for Participant: brno_13\n",
      "cannot convert float NaN to integer\n",
      "Processing data for Participant: brno_14\n",
      "Extracted features saved to 'brno_features/brno_features_brno_14.csv'.\n",
      "Processing data for Participant: brno_15\n",
      "Extracted features saved to 'brno_features/brno_features_brno_15.csv'.\n",
      "Processing data for Participant: brno_16\n",
      "Extracted features saved to 'brno_features/brno_features_brno_16.csv'.\n",
      "Processing data for Participant: brno_17\n",
      "Extracted features saved to 'brno_features/brno_features_brno_17.csv'.\n",
      "Processing completed. 3 participants failed to process.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "\n",
    "def extract_pqrst_features(ecg_signal, sample_rate):\n",
    "    cleaned_ecg = nk.ecg_clean(ecg_signal, sampling_rate=sample_rate)\n",
    "    _, rpeaks = nk.ecg_peaks(cleaned_ecg, sampling_rate=sample_rate)\n",
    "    _, waves_peak = nk.ecg_delineate(cleaned_ecg, rpeaks, sampling_rate=sample_rate, method=\"peak\")\n",
    "\n",
    "    # Initialize dictionary to store features\n",
    "    features = {}\n",
    "\n",
    "    # Check and calculate amplitude differences where indices are valid\n",
    "    def calculate_amplitude_differences(peak_indices, reference_peak_indices):\n",
    "        valid_indices = [i for i in range(len(peak_indices)) if not math.isnan(peak_indices[i]) and not math.isnan(reference_peak_indices[i])]\n",
    "        amplitudes = np.array([cleaned_ecg[int(peak_indices[i])] - cleaned_ecg[int(reference_peak_indices[i])] for i in valid_indices])\n",
    "        return np.mean(amplitudes[~np.isnan(amplitudes)]) if amplitudes.size > 0 else np.nan\n",
    "\n",
    "    # Calculate amplitude differences for P, Q, S, T peaks with respect to R-peaks\n",
    "    features['P_minus_R_amp_mean'] = calculate_amplitude_differences(waves_peak['ECG_P_Peaks'], rpeaks['ECG_R_Peaks']) / 1000  # convert to mV\n",
    "    features['Q_minus_R_amp_mean'] = calculate_amplitude_differences(waves_peak['ECG_Q_Peaks'], rpeaks['ECG_R_Peaks']) / 1000\n",
    "    features['S_minus_R_amp_mean'] = calculate_amplitude_differences(waves_peak['ECG_S_Peaks'], rpeaks['ECG_R_Peaks']) / 1000\n",
    "    features['T_minus_R_amp_mean'] = calculate_amplitude_differences(waves_peak['ECG_T_Peaks'], rpeaks['ECG_R_Peaks']) / 1000\n",
    "\n",
    "    # Calculate interval features\n",
    "    def calculate_intervals(start_peaks, end_peaks):\n",
    "        valid_indices = [i for i in range(len(start_peaks)) if not math.isnan(start_peaks[i]) and not math.isnan(end_peaks[i])]\n",
    "        intervals = np.array([(end_peaks[i] - start_peaks[i]) / sample_rate for i in valid_indices])  # convert to seconds\n",
    "        return np.mean(intervals[~np.isnan(intervals)]) if intervals.size > 0 else np.nan\n",
    "\n",
    "    features['PQ_interval_mean'] = calculate_intervals(waves_peak['ECG_P_Onsets'], waves_peak['ECG_Q_Peaks'])\n",
    "    features['QR_interval_mean'] = calculate_intervals(waves_peak['ECG_Q_Peaks'], rpeaks['ECG_R_Peaks'])\n",
    "    features['RS_interval_mean'] = calculate_intervals(rpeaks['ECG_R_Peaks'], waves_peak['ECG_S_Peaks'])\n",
    "    features['ST_interval_mean'] = calculate_intervals(waves_peak['ECG_S_Peaks'], waves_peak['ECG_T_Peaks'])\n",
    "\n",
    "    return features\n",
    "\n",
    "import math\n",
    "\n",
    "def process_ecg_data(df_ecg, sample_rate):\n",
    "    # Determine the number of samples per 20 seconds\n",
    "    samples_per_minute = sample_rate * 20\n",
    "\n",
    "    # Prepare to collect all features\n",
    "    all_features = []\n",
    "                \n",
    "    for _, group in df_ecg.groupby('Participant'):\n",
    "        for i in range(0, len(group), samples_per_minute):\n",
    "            ecg_segment = group.iloc[i:i+samples_per_minute]\n",
    "            if len(ecg_segment) == samples_per_minute:\n",
    "                features = extract_pqrst_features(ecg_segment['ECG'].values, sample_rate)\n",
    "                features.update({\n",
    "                    'Participant': group['Participant'].iloc[0],\n",
    "                    'Sample': i,\n",
    "                    'Sampling_Rate': sample_rate,\n",
    "                    'Database': group['Database'].iloc[0],\n",
    "                    'Gender': group['Gender'].iloc[0],\n",
    "                    'Age': group['Age'].iloc[0]\n",
    "                })\n",
    "                all_features.append(features)\n",
    "\n",
    "    return pd.DataFrame(all_features)\n",
    "\n",
    "\n",
    "df_ecg = pd.read_csv(\"brno_ECGs.csv\")\n",
    "\n",
    "# drop NaNs\n",
    "df_ecg = df_ecg.dropna()\n",
    "\n",
    "# Assume the sampling rate needs to be defined\n",
    "sample_rate = 1000  # Define the correct sample rate for your data\n",
    "\n",
    "# Process and extract features from the ECG data\n",
    "# features_df = process_ecg_data(df_ecg, sample_rate)\n",
    "\n",
    "# Process and save features by participant in df_ecg, save each user's data in a separate CSV file\n",
    "participants = df_ecg['Participant'].unique()\n",
    "# Report how many participants are being processed and how many failed\n",
    "pcount = len(participants)\n",
    "print(f\"Processing data for {pcount} participants.\")\n",
    "failed = 0\n",
    "for participant in participants:\n",
    "    print(f\"Processing data for Participant: {participant}\")\n",
    "    participant_data = df_ecg[df_ecg['Participant'] == participant]\n",
    "    try:\n",
    "        features = process_ecg_data(participant_data, sample_rate)\n",
    "        # Save the extracted features to a new CSV file\n",
    "        features.to_csv(f\"brno_features/brno_features_{participant}.csv\", index=False)\n",
    "        print(f\"Extracted features saved to 'brno_features/brno_features_{participant}.csv'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data for Participant: {participant}\")\n",
    "        print(e)\n",
    "        failed += 1\n",
    "print(f\"Processing completed. {failed} participants failed to process.\")\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# # Save the extracted features to a new CSV file\n",
    "# features_df.to_csv(\"smart_features.csv\", index=False)\n",
    "# print(\"Extracted features saved to 'smart_features.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dp = pd.read_csv(\"brno-ECGs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ECG</th>\n",
       "      <th>Participant</th>\n",
       "      <th>Sample</th>\n",
       "      <th>Sampling_Rate</th>\n",
       "      <th>Database</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.971089</td>\n",
       "      <td>118001</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.970362</td>\n",
       "      <td>118001</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.971816</td>\n",
       "      <td>118001</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34.971816</td>\n",
       "      <td>118001</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58.003515</td>\n",
       "      <td>118001</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>58.003515</td>\n",
       "      <td>118001</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>46.002788</td>\n",
       "      <td>118001</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22.971089</td>\n",
       "      <td>118001</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22.971089</td>\n",
       "      <td>118001</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>46.002788</td>\n",
       "      <td>118001</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>46.002788</td>\n",
       "      <td>118001</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>58.003515</td>\n",
       "      <td>118001</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>46.002788</td>\n",
       "      <td>118001</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>22.971089</td>\n",
       "      <td>118001</td>\n",
       "      <td>13</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>46.002788</td>\n",
       "      <td>118001</td>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>34.971816</td>\n",
       "      <td>118001</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>70.004243</td>\n",
       "      <td>118001</td>\n",
       "      <td>16</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>70.004243</td>\n",
       "      <td>118001</td>\n",
       "      <td>17</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>70.004243</td>\n",
       "      <td>118001</td>\n",
       "      <td>18</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>58.003515</td>\n",
       "      <td>118001</td>\n",
       "      <td>19</td>\n",
       "      <td>100</td>\n",
       "      <td>brno</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ECG  Participant  Sample  Sampling_Rate Database\n",
       "0   22.971089       118001       0            100     brno\n",
       "1   10.970362       118001       1            100     brno\n",
       "2   34.971816       118001       2            100     brno\n",
       "3   34.971816       118001       3            100     brno\n",
       "4   58.003515       118001       4            100     brno\n",
       "5   58.003515       118001       5            100     brno\n",
       "6   46.002788       118001       6            100     brno\n",
       "7   22.971089       118001       7            100     brno\n",
       "8   22.971089       118001       8            100     brno\n",
       "9   46.002788       118001       9            100     brno\n",
       "10  46.002788       118001      10            100     brno\n",
       "11  58.003515       118001      11            100     brno\n",
       "12  46.002788       118001      12            100     brno\n",
       "13  22.971089       118001      13            100     brno\n",
       "14  46.002788       118001      14            100     brno\n",
       "15  34.971816       118001      15            100     brno\n",
       "16  70.004243       118001      16            100     brno\n",
       "17  70.004243       118001      17            100     brno\n",
       "18  70.004243       118001      18            100     brno\n",
       "19  58.003515       118001      19            100     brno"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'record' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrecord\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'record' is not defined"
     ]
    }
   ],
   "source": [
    "record"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
